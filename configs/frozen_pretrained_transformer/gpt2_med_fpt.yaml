task_name: "pre-training"
run_name: "FPT - GPT2"
random_seed : 13
debug: False

# Model parameters
model_name: "gpt2-medium" # gpt, gpt2-medium
transformer_backbone: "gpt2-medium" # 'gpt2', 'gpt2-medium'
pretraining_run_name: null # None for Huggingface models
pretraining_opt_steps: null # None for Huggingface models
randomly_initialize_backbone: False
return_last_only: True
input_layer_sizes: []
output_layer_sizes: []

# Training parameters
batch_size: 8
learning_rate: 0.001 # 1e-3
num_epochs: 1000
steps_per_epoch: 500 # 500, 100
dropout: 0.1
orth_gain: 1.41

# # FPT datasets parameters
# input_dim: 16 # 48 for CIFAR-10, 16 for MNIST, 50 for BitMemory, 1 for BitXOR, 32128 for IMDB
# output_dim: 10 # 10 for CIFAR, MNIST, 100 for BitMemory, 2 for BitXOR, 2 for IMDB
# use_embeddings_for_input: False # True for IMDB

# FPT experiment parameters
freeze_transformer_backbone: True
freeze_layer_norm: False
freeze_pos: False
freeze_ff: True
freeze_attn: True
