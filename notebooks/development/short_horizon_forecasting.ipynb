{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from tqdm import tqdm\n",
    "\n",
    "from moment.common import PATHS\n",
    "from moment.utils.config import Config\n",
    "from moment.utils.utils import parse_config, dtype_map\n",
    "from moment.utils.forecasting_metrics import get_forecasting_metrics, sMAPELoss\n",
    "from moment.data.dataloader import get_timeseries_dataloader\n",
    "from moment.data.forecasting_datasets import get_forecasting_datasets, ShortForecastingDataset\n",
    "from moment.models.base import BaseModel\n",
    "from moment.models.moment import MOMENT\n",
    "from moment.models.nbeats import NBEATS\n",
    "from moment.models.timesnet import TimesNet\n",
    "from moment.models.gpt4ts import GPT4TS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find relevant runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"MOMENT\"\n",
    "\n",
    "NOTES = {\n",
    "    \"MOMENT\": \"Fine-tune MOMENT on source short-horizon forecasting datasets\",\n",
    "    \"NBEATS\": \"Train N-BEATS on source short-horizon forecasting datasets\",\n",
    "    \"TimesNet\": \"Train TimesNet on source short-horizon forecasting datasets\",\n",
    "    \"GPT4TS\": \"Train GPT4TS on source short-horizon forecasting datasets\"\n",
    "}\n",
    "\n",
    "runs_summary = pd.read_csv(\"../../assets/data/wandb_runs_summary.csv\")\n",
    "runs = runs_summary[runs_summary[\"notes\"] == NOTES[MODEL_NAME]]\n",
    "runs = runs[['model_name', 'run_name', 'hostname', 'dataset_names']]\n",
    "\n",
    "runs.dataset_names = runs.dataset_names.apply(lambda x: x.split(\"/\")[-1])\n",
    "runs.hostname = runs.hostname.apply(lambda x: x.split(\".\")[0])\n",
    "runs['source_frequency'] = runs.dataset_names.apply(lambda x: x.split(\".\")[0].split('_')[1])\n",
    "runs['source_collection'] = runs.dataset_names.apply(lambda x: x.split(\".\")[0].split('_')[0])\n",
    "runs = runs[['model_name', 'run_name', 'source_frequency', 'source_collection']]\n",
    "print(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_forecasting_datasets = get_forecasting_datasets(collection=\"monash\")\n",
    "fred_forecasting_datasets = get_forecasting_datasets(collection=\"fred/preprocessed\")\n",
    "\n",
    "print(\"M3 datasets:\")\n",
    "m_datasets_base_path = '/'.join(short_forecasting_datasets[0].split('/')[:-1])\n",
    "# print(f\"--- M3 & M4 datasets (base path): {m_datasets_base_path}\")\n",
    "print(\"--- M3 splits:\", [i.split('/')[-1] for i in short_forecasting_datasets if \"m3\" in i])\n",
    "print(\"--- M4 splits:\", [i.split('/')[-1] for i in short_forecasting_datasets if \"m4\" in i])\n",
    "\n",
    "print(\"Fred datasets:\")\n",
    "fred_datasets_base_path = '/'.join(fred_forecasting_datasets[0].split('/')[:-1])\n",
    "# print(f\"--- FRED datasets (base path): {fred_datasets_base_path}\")\n",
    "print('--- Splits:', [i.split('/')[-1] for i in fred_forecasting_datasets if \"fred\" in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(args):\n",
    "    args.dataset_names = args.full_file_path_and_name\n",
    "    args.data_split = 'train'\n",
    "    args.batch_size = args.train_batch_size\n",
    "    train_dataloader = get_timeseries_dataloader(args=args)\n",
    "    args.data_split = 'test'\n",
    "    args.batch_size = args.val_batch_size\n",
    "    test_dataloader = get_timeseries_dataloader(args=args)\n",
    "    args.data_split = 'val'\n",
    "    args.batch_size = args.val_batch_size\n",
    "    val_dataloader = get_timeseries_dataloader(args=args)\n",
    "    return train_dataloader, test_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "HORIZON_MAPPING = {'hourly': 48, 'daily': 14, 'weekly': 13, 'monthly': 18, 'quarterly': 8, 'other': 8, 'yearly': 6}\n",
    "\n",
    "def validation(args, model, data_loader, return_preds):\n",
    "    trues, preds, histories, losses = [], [], [], []\n",
    "\n",
    "    # criterion = nn.MSELoss(reduction='mean')\n",
    "    criterion = sMAPELoss(reduction='mean')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_x in tqdm(data_loader, total=len(data_loader)):\n",
    "            timeseries = batch_x.timeseries.float().to(args.device)\n",
    "            input_mask = batch_x.input_mask.long().to(args.device)\n",
    "            forecast = batch_x.forecast.float().to(args.device)\n",
    "            forecast_horizon = forecast.shape[-1]\n",
    "\n",
    "            # scaler = torch.max(timeseries, dim=-1, keepdim=True)[0]\n",
    "            # timeseries = timeseries / scaler\n",
    "\n",
    "            with torch.autocast(device_type='cuda', \n",
    "                                dtype=dtype_map(args.torch_dtype), \n",
    "                                enabled=args.use_amp):\n",
    "                # outputs = model.long_forecast(x_enc=timeseries, \n",
    "                #                         input_mask=input_mask, \n",
    "                #                         mask=None)\n",
    "                outputs = model(x_enc=timeseries, input_mask=input_mask, mask=None)\n",
    "                # outputs.forecast = outputs.forecast * scaler\n",
    "\n",
    "            if outputs.forecast.shape != forecast:\n",
    "                outputs.forecast = outputs.forecast[:, :, :forecast_horizon]\n",
    "                \n",
    "            loss = criterion(outputs.forecast, forecast)                \n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if return_preds:\n",
    "                trues.append(forecast.detach().cpu().numpy())\n",
    "                preds.append(outputs.forecast.detach().cpu().numpy())\n",
    "                histories.append(timeseries.detach().cpu().numpy())\n",
    "    \n",
    "    losses = np.array(losses)\n",
    "    average_loss = np.average(losses)\n",
    "    model.train()\n",
    "\n",
    "    if return_preds:\n",
    "        trues = np.concatenate(trues, axis=0)\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        histories = np.concatenate(histories, axis=0)\n",
    "        return average_loss, losses, (trues, preds, histories)\n",
    "    else:\n",
    "        return average_loss\n",
    "\n",
    "config_file_map = {\n",
    "    \"MOMENT\": \"../../configs/forecasting/linear_probing_short_horizon.yaml\",\n",
    "    \"N-BEATS\": \"../../configs/forecasting/nbeats.yaml\",\n",
    "    \"TimesNet\": \"../../configs/forecasting/timesnet.yaml\",\n",
    "    \"GPT4TS\": \"../../configs/forecasting/gpt4ts.yaml\",\n",
    "}\n",
    "\n",
    "def get_model(model_name, args):\n",
    "    if model_name == \"MOMENT\":\n",
    "        model = MOMENT(args)\n",
    "    elif model_name == \"TimesNet\":\n",
    "        model = TimesNet(args)\n",
    "    elif model_name == \"GPT4TS\":\n",
    "        model = GPT4TS(args)\n",
    "    elif model_name == \"N-BEATS\":\n",
    "        model = NBEATS(args)\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not found.\")\n",
    "    return model\n",
    "\n",
    "def get_checkpoint_name(model_name):\n",
    "    if model_name == \"MOMENT\":\n",
    "        return 'MOMENT.pth'\n",
    "    elif model_name == \"TimesNet\":\n",
    "        return 'TimesNet.pth'\n",
    "    elif model_name == \"GPT4TS\":\n",
    "        return 'GPT4TS.pth'\n",
    "    elif model_name == \"N-BEATS\":\n",
    "        return 'NBEATS.pth'\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG_PATH = \"../../configs/default.yaml\"\n",
    "GPU_ID = 0\n",
    "\n",
    "DATASET = \"m3\" # \"m3\" | \"m4\" | \"fred\"\n",
    "FREQUENCY = \"quarterly\" # \"monthly\" | \"quarterly\" | \"yearly\" | \"daily\" | \"hourly\" | \"weekly\" | \"other\"\n",
    "\n",
    "SOURCE_COLLECTION = \"m4\" # m4 fred\n",
    "SOURCE_FREQUENCY = \"quarterly\" # monthly quarterly yearly daily hourly weekly\n",
    "\n",
    "BASE_PATH = m_datasets_base_path if DATASET in ['m3', 'm4'] else fred_datasets_base_path\n",
    "\n",
    "# linear_probing_short_horizon.yaml\n",
    "config = Config(config_file_path=config_file_map[MODEL_NAME], \n",
    "                default_config_file_path=DEFAULT_CONFIG_PATH).parse()\n",
    "config['device'] = GPU_ID if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = parse_config(config)\n",
    "\n",
    "file_format = 'tsf' if DATASET in ['m3', 'm4'] else 'npy'\n",
    "args.full_file_path_and_name = os.path.join(BASE_PATH, f\"{DATASET}_{FREQUENCY}_dataset.{file_format}\")    \n",
    "args.dataset_names = args.full_file_path_and_name\n",
    "args.forecast_horizon = HORIZON_MAPPING[SOURCE_FREQUENCY] # HORIZON_MAPPING[FREQUENCY]\n",
    "args.val_batch_size = 128\n",
    "\n",
    "train_dataloader, test_dataloader, val_dataloader = get_dataloaders(args)\n",
    "print(f\"Source forecast horizon: {train_dataloader.dataset.forecast_horizon}\")\n",
    "print(f\"Lengths: Train: {train_dataloader.dataset.length_dataset} | Test: {test_dataloader.dataset.length_dataset} | Val: {val_dataloader.dataset.length_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'vibrant-glitter-1232'\n",
    "# selected_runs = runs.loc[(runs['source_collection'] == SOURCE_COLLECTION) & (runs['source_frequency'] == SOURCE_FREQUENCY)]\n",
    "# if selected_runs.shape[0] > 1: \n",
    "#     print(f\"More than one run found for the given source collection and frequency\\n{selected_runs}\")\n",
    "#     selected_runs.loc[:, 'run_num'] = selected_runs.run_name.apply(lambda x: int(x.split(\"-\")[-1]))\n",
    "#     selected_runs = selected_runs.sort_values(by=['run_num'], ascending=False)\n",
    "# checkpoint_name = str(selected_runs.run_name.values[0])\n",
    "print(f\"Checkpoint name: {checkpoint_name}\")\n",
    "\n",
    "model = get_model(MODEL_NAME, args)\n",
    "\n",
    "with open(os.path.join(PATHS.CHECKPOINTS_DIR, f'{checkpoint_name}/{get_checkpoint_name(MODEL_NAME)}'), 'rb') as f:\n",
    "    checkpoint = torch.load(f, map_location='cpu')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, (trues_val, preds_val, _) = validation(args, model, val_dataloader, return_preds=True)\n",
    "_, _, (trues_test, preds_test, _) = validation(args, model, test_dataloader, return_preds=True)\n",
    "trues = np.concatenate([trues_val, trues_test], axis=0)\n",
    "preds = np.concatenate([preds_val, preds_test], axis=0)\n",
    "\n",
    "get_forecasting_metrics(y=trues, y_hat=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_moment(args,\n",
    "                         pretraining_task_name: str = \"pre-training\"):\n",
    "    args.task_name = pretraining_task_name\n",
    "        \n",
    "    checkpoint = BaseModel.load_pretrained_weights(\n",
    "        run_name=args.pretraining_run_name, \n",
    "        opt_steps=args.pretraining_opt_steps)\n",
    "    \n",
    "    pretrained_model = MOMENT(configs=args)\n",
    "    pretrained_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "    return pretrained_model\n",
    "\n",
    "# def validation(args, model, data_loader, return_preds):\n",
    "#     trues, preds, histories, losses = [], [], [], []\n",
    "\n",
    "#     # criterion = nn.MSELoss(reduction='mean')\n",
    "#     criterion = sMAPELoss(reduction='mean')\n",
    "    \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch_x in tqdm(data_loader, total=len(data_loader)):\n",
    "#             timeseries = batch_x.timeseries.float().to(args.device)\n",
    "#             input_mask = batch_x.input_mask.long().to(args.device)\n",
    "#             forecast = batch_x.forecast.float().to(args.device)\n",
    "#             forecast_horizon = forecast.shape[-1]\n",
    "\n",
    "#             # scaler = torch.max(timeseries, dim=-1, keepdim=True)[0]\n",
    "#             # timeseries = timeseries / scaler\n",
    "\n",
    "#             with torch.autocast(device_type='cuda', \n",
    "#                                 dtype=dtype_map(args.torch_dtype), \n",
    "#                                 enabled=args.use_amp): \n",
    "#                 outputs = model.short_forecast(\n",
    "#                     x_enc=timeseries, input_mask=input_mask, forecast_horizon=forecast_horizon)\n",
    "                \n",
    "#                 # outputs.forecast = outputs.forecast * scaler\n",
    "    \n",
    "#             loss = criterion(outputs.forecast, forecast)                \n",
    "#             losses.append(loss.item())\n",
    "\n",
    "#             if return_preds:\n",
    "#                 trues.append(forecast.detach().cpu().numpy())\n",
    "#                 preds.append(outputs.forecast.detach().cpu().numpy())\n",
    "#                 histories.append(timeseries.detach().cpu().numpy())\n",
    "    \n",
    "#     losses = np.array(losses)\n",
    "#     average_loss = np.average(losses)\n",
    "#     model.train()\n",
    "\n",
    "#     if return_preds:\n",
    "#         trues = np.concatenate(trues, axis=0)\n",
    "#         preds = np.concatenate(preds, axis=0)\n",
    "#         histories = np.concatenate(histories, axis=0)\n",
    "#         return average_loss, losses, (trues, preds, histories)\n",
    "#     else:\n",
    "#         return average_loss\n",
    "\n",
    "def validation(args, model, data_loader, return_preds, season, period):\n",
    "    trues, preds, histories, losses = [], [], [], []\n",
    "\n",
    "    # criterion = nn.MSELoss(reduction='mean')\n",
    "    criterion = sMAPELoss(reduction='mean')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_x in tqdm(data_loader, total=len(data_loader)):\n",
    "            timeseries = batch_x.timeseries.float().to(args.device)\n",
    "            input_mask = batch_x.input_mask.long().to(args.device)\n",
    "            forecast = batch_x.forecast.float().to(args.device)\n",
    "            forecast_horizon = forecast.shape[-1]\n",
    "\n",
    "            timeseries = batch_x.timeseries.squeeze().numpy()\n",
    "            decomposition = STL(timeseries, seasonal=season, period=period).fit()\n",
    "            # timeseries = np.concatenate(\n",
    "            #     [decomposition.trend[np.newaxis, :], \n",
    "            #      decomposition.seasonal[np.newaxis, :], \n",
    "            #      decomposition.resid[np.newaxis, :],], axis=0)\n",
    "            timeseries = np.concatenate(\n",
    "                [decomposition.trend[np.newaxis, :], \n",
    "                 decomposition.seasonal[np.newaxis, :],], axis=0)\n",
    "            timeseries = torch.from_numpy(timeseries).unsqueeze(1).float().to(args.device)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', \n",
    "                                dtype=dtype_map(args.torch_dtype), \n",
    "                                enabled=args.use_amp): \n",
    "                outputs = model.short_forecast(\n",
    "                    x_enc=timeseries, input_mask=input_mask, forecast_horizon=forecast_horizon)\n",
    "                \n",
    "            outputs.forecast = outputs.forecast.sum(dim=0, keepdim=True)\n",
    "            loss = criterion(outputs.forecast, forecast)                \n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if return_preds:\n",
    "                trues.append(forecast.detach().cpu().numpy())\n",
    "                preds.append(outputs.forecast.detach().cpu().numpy())\n",
    "                histories.append(timeseries.detach().cpu().numpy())\n",
    "    \n",
    "    losses = np.array(losses)\n",
    "    average_loss = np.average(losses)\n",
    "    model.train()\n",
    "\n",
    "    if return_preds:\n",
    "        trues = np.concatenate(trues, axis=0)\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        histories = np.concatenate(histories, axis=0)\n",
    "        return average_loss, losses, (trues, preds, histories)\n",
    "    else:\n",
    "        return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG_PATH = \"../../configs/default.yaml\"\n",
    "GPU_ID = 0\n",
    "\n",
    "DATASET = \"m4\" # \"m3\" | \"m4\" | \"fred\"\n",
    "FREQUENCY = \"yearly\" # \"monthly\" | \"quarterly\" | \"yearly\" | \"daily\" | \"hourly\" | \"weekly\" | \"other\"\n",
    "BASE_PATH = m_datasets_base_path if DATASET in ['m3', 'm4'] else fred_datasets_base_path\n",
    "SEASON = 25\n",
    "PERIOD = 2\n",
    "\n",
    "# M3 yearly:    Season 25 Period 2\n",
    "# M3 quarterly: Season 3 Period 4\n",
    "# M3 monthly:   Season 11 Period 12\n",
    "# M3 other:     Season 7 Period 7\n",
    "\n",
    "config = Config(config_file_path=\"../../configs/forecasting/zero_shot.yaml\", \n",
    "                default_config_file_path=DEFAULT_CONFIG_PATH).parse()\n",
    "config['device'] = GPU_ID if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = parse_config(config)\n",
    "\n",
    "file_format = 'tsf' if DATASET in ['m3', 'm4'] else 'npy'\n",
    "args.full_file_path_and_name = os.path.join(BASE_PATH, f\"{DATASET}_{FREQUENCY}_dataset.{file_format}\")    \n",
    "args.dataset_names = args.full_file_path_and_name\n",
    "args.forecast_horizon = HORIZON_MAPPING[FREQUENCY] \n",
    "args.val_batch_size = 1\n",
    "\n",
    "train_dataloader, test_dataloader, val_dataloader = get_dataloaders(args)\n",
    "print(f\"Source forecast horizon: {train_dataloader.dataset.forecast_horizon}\")\n",
    "print(f\"Lengths: Train: {train_dataloader.dataset.length_dataset} | Test: {test_dataloader.dataset.length_dataset} | Val: {val_dataloader.dataset.length_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_pretrained_moment(args)\n",
    "model.to(args.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, _, (trues_val, preds_val, _) = validation(args, model, val_dataloader, return_preds=True, season=SEASON, period=PERIOD)\n",
    "# _, _, (trues_test, preds_test, _) = validation(args, model, test_dataloader, return_preds=True, season=SEASON, period=PERIOD)\n",
    "# trues = np.concatenate([trues_val, trues_test], axis=0)\n",
    "# preds = np.concatenate([preds_val, preds_test], axis=0)\n",
    "\n",
    "# get_forecasting_metrics(y=trues, y_hat=preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
