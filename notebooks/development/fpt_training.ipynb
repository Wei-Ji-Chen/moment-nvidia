{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to script fpt_training.ipynb\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "from moment.utils.config import Config\n",
    "from moment.utils.utils import parse_config, control_randomness\n",
    "from moment.data.image_datasets import CIFAR10GrayDataset, CIFAR10Dataset, MNISTDataset\n",
    "from moment.data.bit_datasets import BitMemoryDataset, BitXORDataset\n",
    "from moment.data.nlp_datasets import NLPDataset\n",
    "from moment.models.fpt import FrozenPretrainedTransformer\n",
    "from moment.scripts.development.fpt_trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Randomness needs to be controlled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_ID = 2\n",
    "DEFAULT_CONFIG_PATH = \"../../configs/default.yaml\"\n",
    "\n",
    "# FPT model config\n",
    "config_file_path = \"../../configs/frozen_pretrained_transformer/moment_large_fpt.yaml\"\n",
    "# config_file_path = \"../../configs/frozen_pretrained_transformer/flant5_large_fpt.yaml\"\n",
    "# config_file_path = \"../../configs/frozen_pretrained_transformer/gpt2_med_fpt.yaml\"\n",
    "\n",
    "config = Config(config_file_path=config_file_path, \n",
    "                default_config_file_path=DEFAULT_CONFIG_PATH).parse()\n",
    "config['device'] = GPU_ID if torch.cuda.is_available() else 'cpu'\n",
    "args = parse_config(config)\n",
    "control_randomness(args.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPT = FrozenPretrainedTransformer(configs=args)\n",
    "FPT = FPT.to(args.device)\n",
    "if sys.version_info <= (3, 10):\n",
    "    print('Compiling FPT model...')\n",
    "    FPT = torch.compile(FPT)\n",
    "FPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MNISTDataset(batch_size=args.batch_size, patch_size=4, device=f'cuda:{GPU_ID}')\n",
    "dataset = CIFAR10Dataset(batch_size=args.batch_size, patch_size=4, device=f'cuda:{GPU_ID}')\n",
    "# dataset = BitMemoryDataset(n=1000, num_patterns=5, device=f'cuda:{GPU_ID}')\n",
    "# dataset = NLPDataset(\n",
    "#     dataset_name='imdb',\n",
    "#     model_name=args.model_name,\n",
    "#     batch_size=args.batch_size,\n",
    "#     device=f'cuda:{GPU_ID}'\n",
    "# )\n",
    "\n",
    "x, y = dataset.get_batch(batch_size=args.batch_size)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# original - CIFAR10, MNIST\n",
    "def loss_fn(out, y, x=None):\n",
    "    out = out[:, 0]\n",
    "    return ce_loss(out, y)\n",
    "\n",
    "def accuracy_fn(preds, true, x=None):\n",
    "    preds = preds[:, 0].argmax(-1)\n",
    "    return (preds == true).mean()\n",
    "\n",
    "# # IMDB\n",
    "# def loss_fn(out, y, x=None):\n",
    "#     return ce_loss(out, y)\n",
    "\n",
    "# def accuracy_fn(preds, true, x=None):\n",
    "#     pred_labels = np.argmax(preds, axis=1)\n",
    "#     correct = np.sum(pred_labels == true)\n",
    "#     accuracy = correct / true.shape[0]\n",
    "#     return accuracy\n",
    "\n",
    "# # Bit-Memory\n",
    "# def loss_fn(out, y, x=None):\n",
    "#     out = torch.reshape(out, (-1, 1000, 2))\n",
    "#     ids = torch.zeros(y.shape).to(device=y.device).long()\n",
    "#     ids[y < 0], ids[y > 0] = 0, 1\n",
    "#     out, ids = torch.reshape(out, (-1, 2)), torch.reshape(ids, (-1,))\n",
    "#     return ce_loss(out, ids)\n",
    "\n",
    "# def accuracy_fn(preds, true, x=None):\n",
    "#     preds = preds.reshape(-1, 1000, 2).argmax(-1) * 2 - 1\n",
    "#     return (np.sign(preds) == np.sign(true)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    FPT,\n",
    "    dataset,\n",
    "    loss_fn,\n",
    "    accuracy_fn=accuracy_fn,\n",
    "    steps_per_epoch=args.steps_per_epoch,\n",
    "    test_steps_per_epoch=int(args.steps_per_epoch * 0.2),\n",
    "    learning_rate=args.learning_rate,\n",
    "    batch_size=args.batch_size,\n",
    "    eval_batch_size=1,\n",
    "    grad_accumulate=1,\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project='Time-series Foundation Model',\n",
    "    name='FPT - CIFAR10 - MOMENT',\n",
    "    config=args,\n",
    "    mode='disabled' if args.debug else 'run',\n",
    ")\n",
    "\n",
    "total_steps = 0\n",
    "for i in range(args.num_epochs):\n",
    "    trainer.train_epoch()\n",
    "    total_steps += args.steps_per_epoch\n",
    "    wandb.log({\n",
    "        'Train Loss': trainer.diagnostics['Average Train Loss'],\n",
    "        'Test Loss': trainer.diagnostics['Test Loss'],\n",
    "        'Train Accuracy': trainer.diagnostics['Train Accuracy'],\n",
    "        'Test Accuracy': trainer.diagnostics['Test Accuracy'],\n",
    "        'Epoch': i,\n",
    "        'Steps': total_steps,\n",
    "    })\n",
    "    print(\n",
    "        f'Epoch {i+1}/{args.num_epochs} ' \\\n",
    "        f'Train Loss: {trainer.diagnostics[\"Average Train Loss\"]:.3f} ' \\\n",
    "        f'Test Loss: {trainer.diagnostics[\"Test Loss\"]:.3f} ' \\\n",
    "        f'Train Accuracy: {trainer.diagnostics[\"Train Accuracy\"]:.3f} ' \\\n",
    "        f'Test Accuracy: {trainer.diagnostics[\"Test Accuracy\"]:.3f}'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
